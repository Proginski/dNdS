/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Nextflow config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Default config options for all compute environments
----------------------------------------------------------------------------------------

    Use as follows:
        nextflow run proginski/dnds_full --A <A NUCL FASTA> --B <B NUCL FASTA(S)> --tree <NEWICK WITH A AND B(s)> --ctl_file <CODEML CTL FILE> --outdir <OUTDIR>

----------------------------------------------------------------------------------------
*/

rbh_img  = 'proginski/denovogenes'
dnds_img = 'proginski/dnds'

plugins {
  id 'nf-validation@0.3.1'
}

// Global default params, used in configs
params {

    // Options
    outdir     = 'dnds_results'
    A          = null
    B          = null
    includeA   = false
    fasta_type = 'NT'
    A_seqlist  = null
    ortho      = null
    tree       = null
    ctl_file   = null
    style      = 'one_omega'
    help       = null
    alignment_batch_size = 10

    // Max resource options
    // Defaults only, expecting to be overwritten
    max_memory                 = '128.GB'
    max_cpus                   = 16
    max_time                   = '240.h'

    config_profile_name        = null
    config_profile_description = null

}

// Load base.config by default for all pipelines
//includeConfig 'conf/base.config'

profiles {

    debug {
        dumpHashes             = true
        process.beforeScript   = 'echo $HOSTNAME'
        cleanup                = false
    }

    test    { includeConfig 'conf/test.config'    }
    test_OF { includeConfig 'conf/test_OF.config' }

    bim {

        /* 
        If you want to change any cluster option, use 'clusterOptions'
        e.g. the built-in "queue= 'bim'" setting does not seem to work fine, but clusterOptions  = "-q bim" does.
        */

        executor {
            name      = 'pbspro'
            queueSize = 95
        }

        process {
            clusterOptions  = "-l walltime=43800:00:00 -q bim"

            // For simple process, executed only once, it is quicker to run it locally than to send it to the cluster.
            withLabel: local_job {
                executor       = 'local'
            }
            withName: ALIGN_FOR_CODEML {
                clusterOptions  = "-l walltime=43800:00:00 -l ncpus=${params.max_cpus} -l mem=${MemoryUnit.of(params.max_memory).toMega()}mb"
            }
        }
    }

    conda {
        conda.enabled          = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    mamba {
        conda.enabled          = true
        conda.useMamba         = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    docker {
        conda.enabled          = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false

        process{
            container = rbh_img
            withLabel: dnds {
                container = dnds_img
            }
        }
        docker{
            enabled         = true
            userEmulation   = true
            cacheDir = "${projectDir}/containers"
        }
    }
    arm {
        docker.runOptions = '-u $(id -u):$(id -g) --platform=linux/amd64'
    }
    singularity {
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false

        process{
            container = "docker://${rbh_img}"
            withLabel: dnds {
                container = "docker://${dnds_img}"
            }
        }
        singularity {
            enabled = true
            autoMounts = true
            cacheDir = "${projectDir}/containers"
        }
    }
    podman {
        podman.enabled         = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    shifter {
        shifter.enabled        = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    charliecloud {
        charliecloud.enabled   = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        apptainer.enabled      = false
    }
    apptainer {
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false

        process{
            container = "docker://${rbh_img}"
            withLabel: dnds {
                container = "docker://${dnds_img}"
            }
        }
        apptainer {
            enabled = true
            autoMounts = true
            cacheDir = "${projectDir}/containers"
        }
    }
    gitpod {
        executor.name          = 'local'
        executor.cpus          = 16
        executor.memory        = 60.GB
    }
}

// Set default registry for Apptainer, Docker, Podman and Singularity independent of -profile
// Will not be used unless Apptainer / Docker / Podman / Singularity are enabled
// Set to your registry if you have a mirror of containers
apptainer.registry   = 'quay.io'
docker.registry      = ''
podman.registry      = 'quay.io'
singularity.registry = 'quay.io'

// Export these variables to prevent local Python/R libraries from conflicting with those in the container
// The JULIA depot path has been adjusted to a fixed path `/usr/local/share/julia` that needs to be used for packages in the container.
// See https://apeltzer.github.io/post/03-julia-lang-nextflow/ for details on that. Once we have a common agreement on where to keep Julia packages, this is adjustable.

env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
    JULIA_DEPOT_PATH = "/usr/local/share/julia"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "pipeline_info/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "pipeline_info/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "pipeline_info/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "pipeline_info/pipeline_dag_${trace_timestamp}.html"
}

manifest {
    name            = 'denovogenes'
    author          = """Paul Roginski"""
    homePage        = 'https://github.com/XXX'
    description     = """Reciprocal best hit."""
    mainScript      = 'main.nf'
    nextflowVersion = '!>=23.04.3'
    version         = '1.0dev'
    doi             = ''
}

// Load modules.config for DSL2 module specific options
// includeConfig 'conf/modules.config'

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

